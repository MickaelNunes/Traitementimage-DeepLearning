{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection automatique d'un vêtement (méthode Lenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1512436991641-6745cdb1723f?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)\n",
    "\n",
    "Photo by [Pietro Jeng](https://unsplash.com/photos/sQVXS8HBPPc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce projet, nous allons appliquer l'algorithme mis en évidence par Yann Lecun sur le jeux de données \"Fashion MNIST\". Essayons par rapport à un algorithme basique de machine learning d'améliorer nos résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importons tout d'abord le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mickaelnunes/bin/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voicice qu'il contient:\n",
    "\n",
    "* 0:\tT-shirt/top\n",
    "* 1:\tTrouser\n",
    "* 2:\tPullover\n",
    "* 3:\tDress\n",
    "* 4:\tCoat\n",
    "* 5:\tSandal\n",
    "* 6:\tShirt\n",
    "* 7:\tSneaker\n",
    "* 8:\tBag\n",
    "* 9:\tAnkle boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut déjà, dans un premier temps, s'amuser à voir les photos qu'il contient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "label_class = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "\n",
    "idx = np.random.randint(X_train.shape[0])\n",
    "\n",
    "plt.imshow(X_train[idx], cmap='gray_r')\n",
    "plt.title(label_class[y_train[idx]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "X_train_norm = X_train/255.\n",
    "X_test_norm = X_test/255.\n",
    "\n",
    "X_train_norm = X_train_norm.reshape(X_train_norm.shape[0], 28, 28, 1)\n",
    "X_test_norm = X_test_norm.reshape(X_test_norm.shape[0], 28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons dorénavant l'architecture LeNet sur ce projet afin de voir si les résultats sont cohérents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D, Conv2D, Flatten, Dense\n",
    "\n",
    "\n",
    "def lenet5():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer C1\n",
    "    model.add(Conv2D(filters=6, name='C1', kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    # Layer S2\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='S2'))\n",
    "    # Layer C3\n",
    "    model.add(Conv2D(filters=16, name='C3', kernel_size=(3, 3), activation='relu'))\n",
    "    # Layer S4\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='S4'))\n",
    "    # Before going into layer C5, we flatten our units\n",
    "    model.add(Flatten())\n",
    "    # Layer C5\n",
    "    model.add(Dense(units=120, activation='relu', name='C5'))\n",
    "    # Layer F6\n",
    "    model.add(Dense(units=84, activation='relu', name='F6'))\n",
    "    # Output layer\n",
    "    model.add(Dense(units=10, activation = 'softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.5983 - acc: 0.7807 - val_loss: 0.4405 - val_acc: 0.8378\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 17s 278us/step - loss: 0.4011 - acc: 0.8518 - val_loss: 0.3799 - val_acc: 0.8621\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 16s 274us/step - loss: 0.3491 - acc: 0.8718 - val_loss: 0.3599 - val_acc: 0.8678\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 17s 290us/step - loss: 0.3197 - acc: 0.8822 - val_loss: 0.3374 - val_acc: 0.8809\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 16s 267us/step - loss: 0.2983 - acc: 0.8893 - val_loss: 0.3389 - val_acc: 0.8748\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 16s 268us/step - loss: 0.2838 - acc: 0.8946 - val_loss: 0.3133 - val_acc: 0.8843\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 0.2662 - acc: 0.9006 - val_loss: 0.3054 - val_acc: 0.8923\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 19s 308us/step - loss: 0.2539 - acc: 0.9052 - val_loss: 0.3097 - val_acc: 0.8870\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 18s 305us/step - loss: 0.2431 - acc: 0.9090 - val_loss: 0.2950 - val_acc: 0.8959\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 18s 302us/step - loss: 0.2323 - acc: 0.9138 - val_loss: 0.2987 - val_acc: 0.8925\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.2224 - acc: 0.9174 - val_loss: 0.3032 - val_acc: 0.8919\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.2143 - acc: 0.9202 - val_loss: 0.2899 - val_acc: 0.8957\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 19s 313us/step - loss: 0.2041 - acc: 0.9232 - val_loss: 0.3025 - val_acc: 0.8937\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 16s 268us/step - loss: 0.1966 - acc: 0.9259 - val_loss: 0.2913 - val_acc: 0.8947\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 19s 310us/step - loss: 0.1868 - acc: 0.9296 - val_loss: 0.2938 - val_acc: 0.9008\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 17s 283us/step - loss: 0.1795 - acc: 0.9324 - val_loss: 0.3027 - val_acc: 0.8972\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 15s 248us/step - loss: 0.1711 - acc: 0.9360 - val_loss: 0.3030 - val_acc: 0.8991\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 0.1662 - acc: 0.9371 - val_loss: 0.3179 - val_acc: 0.8938\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.1578 - acc: 0.9401 - val_loss: 0.3243 - val_acc: 0.8965\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.1505 - acc: 0.9435 - val_loss: 0.3361 - val_acc: 0.8938\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 15s 258us/step - loss: 0.1449 - acc: 0.9453 - val_loss: 0.3304 - val_acc: 0.8986\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.1406 - acc: 0.9474 - val_loss: 0.3186 - val_acc: 0.8991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb3f7511b38>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "model = lenet5()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
    "             TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)]\n",
    "\n",
    "# Finally fit the model\n",
    "model.fit(x=X_train_norm, y=y_train_cat, validation_data=(X_test_norm, y_test_cat), epochs=100, batch_size=64, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etant sur un modèle multiclasse, nous décidons d'utiliser la moyenne comme scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 8s 132us/step\n",
      "accuracy on train with NN: 0.9566\n",
      "10000/10000 [==============================] - 1s 132us/step\n",
      "accuracy on test with NN: 0.8991\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('accuracy on train with NN:', model.evaluate(X_train_norm, y_train_cat)[1])\n",
    "print('accuracy on test with NN:', model.evaluate(X_test_norm, y_test_cat)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos résultats sont vraiment bons! L'algorithme arrive à reconnaitre 9 images sur 10 ce qui représente un excellent résultat. Néanmoins, allons plus loin, et créons artificiellement des images. Cela nous permettra de combattre notre overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, rotation_range=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(X_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "938/937 [==============================] - 18s 19ms/step - loss: 0.6575 - acc: 0.7587 - val_loss: 0.4906 - val_acc: 0.8186\n",
      "Epoch 2/100\n",
      "938/937 [==============================] - 21s 22ms/step - loss: 0.4418 - acc: 0.8353 - val_loss: 0.4255 - val_acc: 0.8488\n",
      "Epoch 3/100\n",
      "938/937 [==============================] - 21s 22ms/step - loss: 0.3887 - acc: 0.8566 - val_loss: 0.4007 - val_acc: 0.8536\n",
      "Epoch 4/100\n",
      "938/937 [==============================] - 22s 23ms/step - loss: 0.3613 - acc: 0.8645 - val_loss: 0.3780 - val_acc: 0.8639\n",
      "Epoch 5/100\n",
      "938/937 [==============================] - 22s 23ms/step - loss: 0.3418 - acc: 0.8737 - val_loss: 0.3795 - val_acc: 0.8567\n",
      "Epoch 6/100\n",
      "938/937 [==============================] - 21s 22ms/step - loss: 0.3266 - acc: 0.8792 - val_loss: 0.3512 - val_acc: 0.8717\n",
      "Epoch 7/100\n",
      "938/937 [==============================] - 22s 24ms/step - loss: 0.3149 - acc: 0.8822 - val_loss: 0.3592 - val_acc: 0.8729\n",
      "Epoch 8/100\n",
      "938/937 [==============================] - 29s 31ms/step - loss: 0.3003 - acc: 0.8871 - val_loss: 0.3275 - val_acc: 0.8782\n",
      "Epoch 9/100\n",
      "938/937 [==============================] - 19s 20ms/step - loss: 0.2947 - acc: 0.8891 - val_loss: 0.3305 - val_acc: 0.8814\n",
      "Epoch 10/100\n",
      "938/937 [==============================] - 19s 20ms/step - loss: 0.2847 - acc: 0.8944 - val_loss: 0.3289 - val_acc: 0.8785\n",
      "Epoch 11/100\n",
      "938/937 [==============================] - 19s 20ms/step - loss: 0.2795 - acc: 0.8950 - val_loss: 0.3097 - val_acc: 0.8867\n",
      "Epoch 12/100\n",
      "938/937 [==============================] - 25s 26ms/step - loss: 0.2729 - acc: 0.8991 - val_loss: 0.3029 - val_acc: 0.8910\n",
      "Epoch 13/100\n",
      "938/937 [==============================] - 23s 24ms/step - loss: 0.2662 - acc: 0.8982 - val_loss: 0.3142 - val_acc: 0.8895\n",
      "Epoch 14/100\n",
      "938/937 [==============================] - 22s 23ms/step - loss: 0.2602 - acc: 0.9018 - val_loss: 0.3148 - val_acc: 0.8857\n",
      "Epoch 15/100\n",
      "938/937 [==============================] - 22s 23ms/step - loss: 0.2568 - acc: 0.9019 - val_loss: 0.3070 - val_acc: 0.8915\n",
      "Epoch 16/100\n",
      "938/937 [==============================] - 22s 23ms/step - loss: 0.2524 - acc: 0.9043 - val_loss: 0.2989 - val_acc: 0.8936\n",
      "Epoch 17/100\n",
      "938/937 [==============================] - 22s 23ms/step - loss: 0.2465 - acc: 0.9067 - val_loss: 0.3049 - val_acc: 0.8945\n",
      "Epoch 18/100\n",
      "938/937 [==============================] - 22s 23ms/step - loss: 0.2431 - acc: 0.9071 - val_loss: 0.3120 - val_acc: 0.8940\n",
      "Epoch 19/100\n",
      "938/937 [==============================] - 22s 23ms/step - loss: 0.2409 - acc: 0.9091 - val_loss: 0.3102 - val_acc: 0.8923\n",
      "Epoch 20/100\n",
      "938/937 [==============================] - 22s 23ms/step - loss: 0.2337 - acc: 0.9123 - val_loss: 0.2997 - val_acc: 0.8918\n",
      "Epoch 21/100\n",
      "938/937 [==============================] - 1874s 2s/step - loss: 0.2316 - acc: 0.9124 - val_loss: 0.2985 - val_acc: 0.8955\n",
      "Epoch 22/100\n",
      "938/937 [==============================] - 24s 26ms/step - loss: 0.2290 - acc: 0.9122 - val_loss: 0.3038 - val_acc: 0.8947\n",
      "Epoch 23/100\n",
      "938/937 [==============================] - 22s 23ms/step - loss: 0.2260 - acc: 0.9147 - val_loss: 0.3019 - val_acc: 0.8942\n",
      "Epoch 24/100\n",
      "938/937 [==============================] - 25s 27ms/step - loss: 0.2242 - acc: 0.9151 - val_loss: 0.2996 - val_acc: 0.8991\n",
      "Epoch 25/100\n",
      "938/937 [==============================] - 23s 24ms/step - loss: 0.2218 - acc: 0.9154 - val_loss: 0.3113 - val_acc: 0.8950\n",
      "Epoch 26/100\n",
      "938/937 [==============================] - 22s 24ms/step - loss: 0.2181 - acc: 0.9170 - val_loss: 0.3007 - val_acc: 0.8982\n",
      "Epoch 27/100\n",
      "938/937 [==============================] - 22s 24ms/step - loss: 0.2139 - acc: 0.9177 - val_loss: 0.3264 - val_acc: 0.8892\n",
      "Epoch 28/100\n",
      "938/937 [==============================] - 21s 23ms/step - loss: 0.2160 - acc: 0.9178 - val_loss: 0.3068 - val_acc: 0.8942\n",
      "Epoch 29/100\n",
      "938/937 [==============================] - 21s 23ms/step - loss: 0.2122 - acc: 0.9200 - val_loss: 0.3135 - val_acc: 0.8952\n",
      "Epoch 30/100\n",
      "938/937 [==============================] - 22s 24ms/step - loss: 0.2100 - acc: 0.9199 - val_loss: 0.2979 - val_acc: 0.8973\n",
      "Epoch 31/100\n",
      "938/937 [==============================] - 22s 23ms/step - loss: 0.2071 - acc: 0.9214 - val_loss: 0.3088 - val_acc: 0.8922\n",
      "Epoch 32/100\n",
      "938/937 [==============================] - 22s 23ms/step - loss: 0.2050 - acc: 0.9216 - val_loss: 0.3126 - val_acc: 0.8921\n",
      "Epoch 33/100\n",
      "938/937 [==============================] - 21s 23ms/step - loss: 0.2057 - acc: 0.9209 - val_loss: 0.3076 - val_acc: 0.8971\n",
      "Epoch 34/100\n",
      "938/937 [==============================] - 21s 23ms/step - loss: 0.2014 - acc: 0.9229 - val_loss: 0.3247 - val_acc: 0.8912\n",
      "Epoch 35/100\n",
      "938/937 [==============================] - 21s 23ms/step - loss: 0.1997 - acc: 0.9240 - val_loss: 0.3016 - val_acc: 0.8994\n",
      "Epoch 36/100\n",
      "938/937 [==============================] - 22s 23ms/step - loss: 0.1946 - acc: 0.9256 - val_loss: 0.3250 - val_acc: 0.8964\n",
      "Epoch 37/100\n",
      "938/937 [==============================] - 23s 25ms/step - loss: 0.1967 - acc: 0.9250 - val_loss: 0.3084 - val_acc: 0.8974\n",
      "Epoch 38/100\n",
      "938/937 [==============================] - 21s 23ms/step - loss: 0.1947 - acc: 0.9253 - val_loss: 0.3048 - val_acc: 0.9030\n",
      "Epoch 39/100\n",
      "938/937 [==============================] - 23s 24ms/step - loss: 0.1927 - acc: 0.9263 - val_loss: 0.3200 - val_acc: 0.9012\n",
      "Epoch 40/100\n",
      "938/937 [==============================] - 25s 26ms/step - loss: 0.1921 - acc: 0.9273 - val_loss: 0.2924 - val_acc: 0.9025\n",
      "Epoch 41/100\n",
      "938/937 [==============================] - 21s 22ms/step - loss: 0.1884 - acc: 0.9292 - val_loss: 0.3009 - val_acc: 0.9025\n",
      "Epoch 42/100\n",
      "938/937 [==============================] - 22s 24ms/step - loss: 0.1881 - acc: 0.9279 - val_loss: 0.3146 - val_acc: 0.9010\n",
      "Epoch 43/100\n",
      "938/937 [==============================] - 24s 26ms/step - loss: 0.1862 - acc: 0.9280 - val_loss: 0.3064 - val_acc: 0.9012\n",
      "Epoch 44/100\n",
      "938/937 [==============================] - 23s 25ms/step - loss: 0.1850 - acc: 0.9296 - val_loss: 0.3230 - val_acc: 0.8956\n",
      "Epoch 45/100\n",
      "938/937 [==============================] - 24s 26ms/step - loss: 0.1849 - acc: 0.9288 - val_loss: 0.3120 - val_acc: 0.8970\n",
      "Epoch 46/100\n",
      "938/937 [==============================] - 20s 21ms/step - loss: 0.1833 - acc: 0.9298 - val_loss: 0.3290 - val_acc: 0.8917\n",
      "Epoch 47/100\n",
      "938/937 [==============================] - 19s 21ms/step - loss: 0.1823 - acc: 0.9303 - val_loss: 0.3047 - val_acc: 0.8997\n",
      "Epoch 48/100\n",
      "938/937 [==============================] - 19s 20ms/step - loss: 0.1778 - acc: 0.9316 - val_loss: 0.3051 - val_acc: 0.9014\n",
      "Epoch 49/100\n",
      "938/937 [==============================] - 20s 21ms/step - loss: 0.1779 - acc: 0.9314 - val_loss: 0.3129 - val_acc: 0.9005\n",
      "Epoch 50/100\n",
      "938/937 [==============================] - 20s 21ms/step - loss: 0.1762 - acc: 0.9319 - val_loss: 0.3045 - val_acc: 0.9043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb3f54a47f0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lenet5()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
    "             TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)]\n",
    "\n",
    "batch_size = 64\n",
    "model.fit_generator(datagen.flow(X_train_norm, y_train_cat, batch_size=batch_size),\n",
    "                    validation_data=(X_test_norm, y_test_cat), callbacks=callbacks,\n",
    "                    steps_per_epoch=len(X_train_norm) / batch_size, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons si grâce à cette méthode, nous arrivons à un score plus élevé?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 8s 135us/step\n",
      "accuracy on train with NN: 0.93875\n",
      "10000/10000 [==============================] - 1s 125us/step\n",
      "accuracy on test with NN: 0.9033\n"
     ]
    }
   ],
   "source": [
    "print('accuracy on train with NN:', model.evaluate(X_train_norm, y_train_cat)[1])\n",
    "print('accuracy on test with NN:', model.evaluate(X_test_norm, y_test_cat)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat s'améliore donc un peu plus malgré le fait qu'on ait pas réussi à combattre l'overfitting. Il faudra donc jouer un peu plus sur les paramètres ou peut-être labéliser plus de photos... Mais finalement pas si mal pour un début!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
